{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/icb/alioguz.can/miniconda3/envs/campa_pt/bin/python\n",
      "gpusrv34.scidom.de\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import socket\n",
    "print(socket.gethostname())\n",
    "import os\n",
    "os.chdir(\"/home/icb/alioguz.can/projects/campa_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/alioguz.can/miniconda3/envs/campa_pt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-15 20:32:30.261846: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-15 20:32:31.601583: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-15 20:32:31.601707: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-15 20:32:31.601720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/icb/alioguz.can/miniconda3/envs/campa_pt/lib/python3.9/site-packages/numba/core/decorators.py:246: RuntimeWarning: nopython is set for njit and is ignored\n",
      "  warnings.warn('nopython is set for njit and is ignored', RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from /ictstr01/home/icb/alioguz.can/projects/campa_pt/notebooks/params/campa.ini\n",
      "CAMPAConfig (fname: /ictstr01/home/icb/alioguz.can/projects/campa_pt/notebooks/params/campa.ini)\n",
      "EXPERIMENT_DIR: /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments\n",
      "BASE_DATA_DIR: /home/icb/alioguz.can/projects/campa_pt/notebooks/example_data\n",
      "CO_OCC_CHUNK_SIZE: 10000000.0\n",
      "data_config/exampledata: /home/icb/alioguz.can/projects/campa_pt/notebooks/params/ExampleData_constants.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from campa.tl import (\n",
    "    Cluster,\n",
    "    Estimator,\n",
    "    TorchEstimator,\n",
    "    Predictor,\n",
    "    TorchPredictor,\n",
    "    Experiment,\n",
    "    TorchExperiment,\n",
    "    ModelComparator,\n",
    "    run_experiments,\n",
    "    run_torch_experiments\n",
    ")\n",
    "from campa.data import MPPData\n",
    "from campa.utils import init_logging\n",
    "from campa.constants import campa_config\n",
    "# init logging with level INFO=20, WARNING=30\n",
    "init_logging(level=30)\n",
    "# read correct campa_config -- created with setup.ipynb\n",
    "CAMPA_DIR = Path.cwd()\n",
    "campa_config.config_fname = CAMPA_DIR / \"notebooks/params/campa.ini\"\n",
    "print(campa_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import backend as K\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# # Clear the current session\n",
    "# K.clear_session()\n",
    "\n",
    "# # Optionally, reset the default graph\n",
    "# tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Experiments from config\n",
    "exps = Experiment.get_experiments_from_config(\"notebooks/params/example_experiment_params.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiments(exps, mode=\"trainval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: CondVAE_pert-CC\n",
      "Experiment is stored in: /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/CondVAE_pert-CC\n"
     ]
    }
   ],
   "source": [
    "exp = exps[1]\n",
    "print(\"Experiment name:\", exp.name)\n",
    "print(\"Experiment is stored in:\", exp.full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 18:43:49.165507: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-01-15 18:43:49.236844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2025-01-15 18:43:49.360194: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "est = Estimator(exps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 10s 346ms/step - loss: 5223.8276 - decoder_loss: 5223.8062 - latent_loss: 0.0221 - decoder_mean_squared_error: 0.7407 - latent_kl_loss: 0.0223 - val_loss: 5243.8989 - val_decoder_loss: 5243.8716 - val_latent_loss: 0.0274 - val_decoder_mean_squared_error: 0.7189 - val_latent_kl_loss: 0.0274\n"
     ]
    }
   ],
   "source": [
    "_ = est.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 419ms/step\n",
      "1/1 [==============================] - 0s 392ms/step\n",
      "319/319 [==============================] - 1s 2ms/step\n",
      "319/319 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_torch = Predictor(exps[1])\n",
    "pred_torch.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VAEModel\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 14)]         0           []                               \n",
      "                                                                                                  \n",
      " condition_encoder (Functional)  (None, 10)          260         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 3, 3, 34)]   0           []                               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 3, 3, 10)     0           ['condition_encoder[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3, 3, 44)     0           ['input_1[0][0]',                \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 3, 3, 32)     1440        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 288)          0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           9248        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " encoder (Functional)           (None, 16)           12020       ['input_1[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 16)           528         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 34)           1178        ['encoder[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " latent (Dense)                 (None, 32)           544         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,938\n",
      "Trainable params: 12,938\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(est.model.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: CondVAE_pert-CC\n",
      "Experiment is stored in: /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/CondVAE_pert-CC\n"
     ]
    }
   ],
   "source": [
    "torch_exps = TorchExperiment.get_experiments_from_config(\"notebooks/params/example_experiment_params_torch.py\")\n",
    "print(\"Experiment name:\", torch_exps[1].name)\n",
    "print(\"Experiment is stored in:\", torch_exps[1].full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAEModelTorch(\n",
      "  (condition_encoder_latent): Sequential(\n",
      "    (0): Linear(in_features=14, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(44, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=288, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (latent): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (condition_encoder_decoder): Sequential(\n",
      "    (0): Linear(in_features=14, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=26, out_features=34, bias=True)\n",
      "  )\n",
      ")\n",
      "13198\n"
     ]
    }
   ],
   "source": [
    "est_torch = TorchEstimator(torch_exps[1])\n",
    "print(est_torch.model)\n",
    "est_torch.model.total_trainable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1/1 [00:00<00:00, 31.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 0,\n",
       " 'loss': 3995.544189453125,\n",
       " 'val_loss': 4118.7421875,\n",
       " 'decoder_loss': 3995.5421142578125,\n",
       " 'latent_loss': 0.0019918173202313483}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_torch.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:TorchEstimator:WARNING: weights_path set to true but no trained model found in /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/VAE\n"
     ]
    }
   ],
   "source": [
    "pred_torch = TorchPredictor(torch_exps[0])\n",
    "pred_torch.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results folder /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/VAE/results_epoch000\n",
      "['val', 'val_imgs']\n"
     ]
    }
   ],
   "source": [
    "results_folder = os.path.join(pred_torch.exp.full_path, f\"results_epoch{pred_torch.est.epoch:03d}\")\n",
    "print(\"Results folder\", results_folder)\n",
    "print(os.listdir(results_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPPData for ExampleData (123 mpps with shape (3, 3, 34) from 8 objects). Data keys: ['x', 'y', 'obj_ids', 'labels', 'mpp', 'conditions', 'latent'].\n"
     ]
    }
   ],
   "source": [
    "print(MPPData.from_data_dir(os.path.join(results_folder, \"val\"), data_config=\"ExampleData\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot read with memmap:  /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/VAE/results_epoch001/val/clustering.npy\n"
     ]
    }
   ],
   "source": [
    "cl = Cluster.from_exp_split(torch_exps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/tl/_cluster.py:598: FutureWarning: In the future, the default backend for leiden will be igraph instead of leidenalg.\n",
      "\n",
      " To achieve the future defaults please pass: flavor=\"igraph\" and n_iterations=2.  directed must also be False to work with igraph's implementation.\n",
      "  sc.tl.leiden(\n",
      "WARNING:MPPData:Saving partial keys of mpp data without a base_data_dir to enable correct loading\n"
     ]
    }
   ],
   "source": [
    "print(cl.config[\"leiden_resolution\"])\n",
    "cl.create_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_torch_experiments(torch_exps, mode=\"trainval\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
