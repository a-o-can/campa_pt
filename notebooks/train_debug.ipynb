{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/icb/alioguz.can/miniconda3/envs/campa_pt/bin/python\n",
      "gpusrv32.scidom.de\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import socket\n",
    "print(socket.gethostname())\n",
    "import os\n",
    "os.chdir(\"/home/icb/alioguz.can/projects/campa_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/alioguz.can/miniconda3/envs/campa_pt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-13 15:55:51.705401: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-13 15:56:49.625948: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-13 15:56:49.640837: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-13 15:56:49.640862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/icb/alioguz.can/miniconda3/envs/campa_pt/lib/python3.9/site-packages/numba/core/decorators.py:246: RuntimeWarning: nopython is set for njit and is ignored\n",
      "  warnings.warn('nopython is set for njit and is ignored', RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from /ictstr01/home/icb/alioguz.can/projects/campa_pt/notebooks/params/campa.ini\n",
      "CAMPAConfig (fname: /ictstr01/home/icb/alioguz.can/projects/campa_pt/notebooks/params/campa.ini)\n",
      "EXPERIMENT_DIR: /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments\n",
      "BASE_DATA_DIR: /home/icb/alioguz.can/projects/campa_pt/notebooks/example_data\n",
      "CO_OCC_CHUNK_SIZE: 10000000.0\n",
      "data_config/exampledata: /home/icb/alioguz.can/projects/campa_pt/notebooks/params/ExampleData_constants.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from campa.tl import (\n",
    "    Cluster,\n",
    "    Estimator,\n",
    "    TorchEstimator,\n",
    "    Predictor,\n",
    "    TorchPredictor,\n",
    "    Experiment,\n",
    "    TorchExperiment,\n",
    "    ModelComparator,\n",
    "    run_experiments,\n",
    "    run_torch_experiments\n",
    ")\n",
    "from campa.data import MPPData\n",
    "from campa.utils import init_logging\n",
    "from campa.constants import campa_config\n",
    "# init logging with level INFO=20, WARNING=30\n",
    "init_logging(level=30)\n",
    "# read correct campa_config -- created with setup.ipynb\n",
    "CAMPA_DIR = Path.cwd()\n",
    "campa_config.config_fname = CAMPA_DIR / \"notebooks/params/campa.ini\"\n",
    "print(campa_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import backend as K\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# # Clear the current session\n",
    "# K.clear_session()\n",
    "\n",
    "# # Optionally, reset the default graph\n",
    "# tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Experiments from config\n",
    "exps = Experiment.get_experiments_from_config(\"notebooks/params/example_experiment_params.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiments(exps, mode=\"trainval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: VAE\n",
      "Experiment is stored in: /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/VAE\n"
     ]
    }
   ],
   "source": [
    "exp = exps[0]\n",
    "print(\"Experiment name:\", exp.name)\n",
    "print(\"Experiment is stored in:\", exp.full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:58:46.263092: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-01-13 15:58:46.353774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2025-01-13 15:58:46.506295: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "est = Estimator(exps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 9s 1s/step - loss: 5397.7578 - decoder_loss: 5397.7217 - latent_loss: 0.0361 - decoder_mean_squared_error: 0.8036 - latent_kl_loss: 0.0355 - val_loss: 5319.9741 - val_decoder_loss: 5319.9424 - val_latent_loss: 0.0319 - val_decoder_mean_squared_error: 0.7455 - val_latent_kl_loss: 0.0319\n"
     ]
    }
   ],
   "source": [
    "_ = est.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7f9a7e3a8130>,\n",
       " 'latent': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7f9a7ed72970>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.model.model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VAEModel\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 3, 34)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 3, 3, 32)     1120        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 288)          0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           9248        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " encoder (Functional)           (None, 16)           11440       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           528         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 34)           578         ['encoder[0][0]']                \n",
      "                                                                                                  \n",
      " latent (Dense)                 (None, 32)           544         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,018\n",
      "Trainable params: 12,018\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(est.model.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_exps = TorchExperiment.get_experiments_from_config(\"notebooks/params/example_experiment_params_torch.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAEModelTorch(\n",
      "  (encoder_input): ModuleList()\n",
      "  (decoder_input): ModuleList()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(34, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=288, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (latent): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=34, bias=True)\n",
      "  )\n",
      ")\n",
      "12018\n"
     ]
    }
   ],
   "source": [
    "est_torch = TorchEstimator(torch_exps[0])\n",
    "print(est_torch.model)\n",
    "est_torch.model.total_trainable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001,\n",
       " 'epochs': 1,\n",
       " 'batch_size': 128,\n",
       " 'loss': {'decoder': <function campa.tl._losses.sigma_vae_mse_torch(y_true, y_pred)>,\n",
       "  'latent': <function campa.tl._losses.kl_loss_torch(y_true, y_pred)>},\n",
       " 'loss_weights': {'decoder': 1, 'latent': 1},\n",
       " 'loss_warmup_to_epoch': {},\n",
       " 'metrics': {'decoder': MSELoss(),\n",
       "  'latent': <function campa.tl._losses.kl_loss_torch(y_true, y_pred)>},\n",
       " 'save_model_weights': True,\n",
       " 'save_history': True,\n",
       " 'overwrite_history': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_torch.config[\"training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_torch.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:TorchEstimator:WARNING: weights_path set to true but no trained model found in /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/VAE\n"
     ]
    }
   ],
   "source": [
    "pred_torch = TorchPredictor(torch_exps[0])\n",
    "pred_torch.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results folder /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/VAE/results_epoch000\n",
      "['val', 'val_imgs']\n"
     ]
    }
   ],
   "source": [
    "results_folder = os.path.join(pred_torch.exp.full_path, f\"results_epoch{pred_torch.est.epoch:03d}\")\n",
    "print(\"Results folder\", results_folder)\n",
    "print(os.listdir(results_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPPData for ExampleData (123 mpps with shape (3, 3, 34) from 8 objects). Data keys: ['obj_ids', 'x', 'y', 'mpp', 'conditions', 'labels', 'latent'].\n"
     ]
    }
   ],
   "source": [
    "print(MPPData.from_data_dir(os.path.join(results_folder, \"val\"), data_config=\"ExampleData\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot read with memmap:  /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/VAE/results_epoch001/val/clustering.npy\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cl \u001b[38;5;241m=\u001b[39m \u001b[43mCluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_exp_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_exps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/tl/_cluster.py:281\u001b[0m, in \u001b[0;36mCluster.from_exp_split\u001b[0;34m(cls, exp)\u001b[0m\n\u001b[1;32m    273\u001b[0m cluster_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    274\u001b[0m cluster_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_data_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    275\u001b[0m     exp\u001b[38;5;241m.\u001b[39mdir,\n\u001b[1;32m    276\u001b[0m     exp\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults_epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    278\u001b[0m     exp\u001b[38;5;241m.\u001b[39mevaluate_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    279\u001b[0m )\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcluster_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/tl/_cluster.py:192\u001b[0m, in \u001b[0;36mCluster.__init__\u001b[0;34m(self, config, cluster_mpp, save_config)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_mpp \u001b[38;5;241m=\u001b[39m cluster_mpp\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# try to load it from disk if not already initialised\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_mpp\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# initialise annotation\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_annotation: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/tl/_cluster.py:301\u001b[0m, in \u001b[0;36mCluster.cluster_mpp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m:class:`MPPData` that is used for clustering.\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03mNone if data could not be loaded.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_mpp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_mpp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_cluster_mpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_mpp\n",
      "File \u001b[0;32m/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/tl/_cluster.py:323\u001b[0m, in \u001b[0;36mCluster._load_cluster_mpp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 323\u001b[0m     mpp_data \u001b[38;5;241m=\u001b[39m \u001b[43mMPPData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcampa_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEXPERIMENT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmpp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mumap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded cluster_mpp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmpp_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mpp_data\n",
      "File \u001b[0;32m/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/data/_data.py:192\u001b[0m, in \u001b[0;36mMPPData.from_data_dir\u001b[0;34m(cls, data_dir, data_config, mode, base_dir, keys, optional_keys, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# print(f\"!!!!!adding from {base_dir}, {data_dir}\")\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# second, add mpp_data\u001b[39;00m\n\u001b[1;32m    191\u001b[0m res_keys, res_optional_keys \u001b[38;5;241m=\u001b[39m _get_keys(keys, optional_keys, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_data_from_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptional_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_optional_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmpp_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, with base data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmpp_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_data_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m=\u001b[39m data_dir\n",
      "File \u001b[0;32m/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/data/_data.py:508\u001b[0m, in \u001b[0;36mMPPData.add_data_from_dir\u001b[0;34m(self, data_dir, keys, optional_keys, subset, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# check that obj_ids from mpp_to_add are the same / a subset of the current obj_ids\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subset:\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_obj_ids)\u001b[38;5;241m.\u001b[39missuperset(mpp_to_add\u001b[38;5;241m.\u001b[39munique_obj_ids)\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# subset self to the obj_ids in mpp_to_add\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubset(obj_ids\u001b[38;5;241m=\u001b[39mmpp_to_add\u001b[38;5;241m.\u001b[39munique_obj_ids)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cl = Cluster.from_exp_split(torch_exps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cl.config[\"leiden_resolution\"])\n",
    "cl.create_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_torch_experiments(torch_exps, mode=\"trainval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
