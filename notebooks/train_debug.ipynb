{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/icb/alioguz.can/miniconda3/envs/campa_pt/bin/python\n",
      "cpusrv30.scidom.de\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import socket\n",
    "print(socket.gethostname())\n",
    "import os\n",
    "os.chdir(\"/home/icb/alioguz.can/projects/campa_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/alioguz.can/miniconda3/envs/campa_pt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-11 16:44:43.052934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-11 16:44:54.938237: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-11 16:45:46.768108: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-11 16:45:46.777800: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-11 16:45:46.777828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/icb/alioguz.can/miniconda3/envs/campa_pt/lib/python3.9/site-packages/numba/core/decorators.py:246: RuntimeWarning: nopython is set for njit and is ignored\n",
      "  warnings.warn('nopython is set for njit and is ignored', RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from /ictstr01/home/icb/alioguz.can/projects/campa_pt/notebooks/params/campa.ini\n",
      "CAMPAConfig (fname: /ictstr01/home/icb/alioguz.can/projects/campa_pt/notebooks/params/campa.ini)\n",
      "EXPERIMENT_DIR: /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments\n",
      "BASE_DATA_DIR: /home/icb/alioguz.can/projects/campa_pt/notebooks/example_data\n",
      "CO_OCC_CHUNK_SIZE: 10000000.0\n",
      "data_config/exampledata: /home/icb/alioguz.can/projects/campa_pt/notebooks/params/ExampleData_constants.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from campa.tl import (\n",
    "    Cluster,\n",
    "    Estimator,\n",
    "    TorchEstimator,\n",
    "    Predictor,\n",
    "    TorchPredictor,\n",
    "    Experiment,\n",
    "    TorchExperiment,\n",
    "    ModelComparator,\n",
    "    run_experiments,\n",
    "    run_torch_experiments\n",
    ")\n",
    "from campa.data import MPPData\n",
    "from campa.utils import init_logging\n",
    "from campa.constants import campa_config\n",
    "# init logging with level INFO=20, WARNING=30\n",
    "init_logging(level=30)\n",
    "# read correct campa_config -- created with setup.ipynb\n",
    "CAMPA_DIR = Path.cwd()\n",
    "campa_config.config_fname = CAMPA_DIR / \"notebooks/params/campa.ini\"\n",
    "print(campa_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import backend as K\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# # Clear the current session\n",
    "# K.clear_session()\n",
    "\n",
    "# # Optionally, reset the default graph\n",
    "# tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Experiments from config\n",
    "exps = Experiment.get_experiments_from_config(\"notebooks/params/example_experiment_params.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiments(exps, mode=\"trainval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: VAE\n",
      "Experiment is stored in: /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/VAE\n"
     ]
    }
   ],
   "source": [
    "exp = exps[0]\n",
    "print(\"Experiment name:\", exp.name)\n",
    "print(\"Experiment is stored in:\", exp.full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-11 16:48:33.783180: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-01-11 16:48:33.799446: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-01-11 16:48:33.800386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cpusrv30.scidom.de): /proc/driver/nvidia/version does not exist\n",
      "2025-01-11 16:48:33.974215: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent before reparam (None, 32)\n",
      "latent after reparam (None, 16)\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 3, 34)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 3, 3, 32)     1120        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 288)          0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           9248        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           528         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " latent (Dense)                 (None, 32)           544         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.split (TFOpLambda)          [(None, 16),         0           ['latent[0][0]']                 \n",
      "                                 (None, 16)]                                                      \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLambda  (2,)                0           ['tf.split[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 16)           0           ['tf.split[0][1]']               \n",
      "                                                                                                  \n",
      " tf.random.normal (TFOpLambda)  (None, 16)           0           ['tf.compat.v1.shape[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.exp (TFOpLambda)       (None, 16)           0           ['tf.math.multiply[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 16)          0           ['tf.random.normal[0][0]',       \n",
      " )                                                                'tf.math.exp[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 16)          0           ['tf.math.multiply_1[0][0]',     \n",
      " da)                                                              'tf.split[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,440\n",
      "Trainable params: 11,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "encoder None\n",
      "latent (None, 32)\n",
      "Decoder input (None, 16)\n",
      "model_output 1, 2 (None, 34) (None, 32)\n"
     ]
    }
   ],
   "source": [
    "est = Estimator(exps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 45ms/step - loss: 5494.1187 - decoder_loss: 5494.1025 - latent_loss: 0.0161 - decoder_mean_squared_error: 0.8435 - latent_kl_loss: 0.0162 - val_loss: 5567.1948 - val_decoder_loss: 5567.1758 - val_latent_loss: 0.0191 - val_decoder_mean_squared_error: 0.8391 - val_latent_kl_loss: 0.0191\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>decoder_loss</th>\n",
       "      <th>latent_loss</th>\n",
       "      <th>decoder_mean_squared_error</th>\n",
       "      <th>latent_kl_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_decoder_loss</th>\n",
       "      <th>val_latent_loss</th>\n",
       "      <th>val_decoder_mean_squared_error</th>\n",
       "      <th>val_latent_kl_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5494.118652</td>\n",
       "      <td>5494.102539</td>\n",
       "      <td>0.01611</td>\n",
       "      <td>0.843513</td>\n",
       "      <td>0.016176</td>\n",
       "      <td>5567.194824</td>\n",
       "      <td>5567.175781</td>\n",
       "      <td>0.019061</td>\n",
       "      <td>0.839096</td>\n",
       "      <td>0.019061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              loss  decoder_loss  latent_loss  decoder_mean_squared_error  \\\n",
       "epoch                                                                       \n",
       "0      5494.118652   5494.102539      0.01611                    0.843513   \n",
       "\n",
       "       latent_kl_loss     val_loss  val_decoder_loss  val_latent_loss  \\\n",
       "epoch                                                                   \n",
       "0            0.016176  5567.194824       5567.175781         0.019061   \n",
       "\n",
       "       val_decoder_mean_squared_error  val_latent_kl_loss  \n",
       "epoch                                                      \n",
       "0                            0.839096            0.019061  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VAEModel\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 3, 3, 34)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 3, 3, 32)     1120        ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 288)          0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 32)           9248        ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " encoder (Functional)           (None, 16)           11440       ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 16)           528         ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 34)           578         ['encoder[0][0]']                \n",
      "                                                                                                  \n",
      " latent (Dense)                 (None, 32)           544         ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,018\n",
      "Trainable params: 12,018\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(est.model.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_exps = TorchExperiment.get_experiments_from_config(\"notebooks/params/example_experiment_params_torch.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAEModelTorch(\n",
      "  (encoder_input): ModuleList()\n",
      "  (decoder_input): ModuleList()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(34, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=288, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (latent): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=34, bias=True)\n",
      "  )\n",
      ")\n",
      "12018\n"
     ]
    }
   ],
   "source": [
    "est_torch = TorchEstimator(torch_exps[0])\n",
    "print(est_torch.model)\n",
    "est_torch.model.total_trainable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/alioguz.can/miniconda3/envs/campa_pt/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:172: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 0, 'loss': 18862.36279296875, 'val_loss': 4765.98974609375}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_torch.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:TorchEstimator:WARNING: weights_path set to true but no trained model found in /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/VAE\n"
     ]
    }
   ],
   "source": [
    "pred_torch = TorchPredictor(torch_exps[0])\n",
    "pred_torch.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results folder /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/VAE/results_epoch000\n",
      "['val', 'val_imgs']\n"
     ]
    }
   ],
   "source": [
    "results_folder = os.path.join(pred_torch.exp.full_path, f\"results_epoch{pred_torch.est.epoch:03d}\")\n",
    "print(\"Results folder\", results_folder)\n",
    "print(os.listdir(results_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPPData for ExampleData (123 mpps with shape (3, 3, 34) from 8 objects). Data keys: ['x', 'obj_ids', 'y', 'labels', 'mpp', 'conditions', 'latent'].\n"
     ]
    }
   ],
   "source": [
    "print(MPPData.from_data_dir(os.path.join(results_folder, \"val\"), data_config=\"ExampleData\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot read with memmap:  /home/icb/alioguz.can/projects/campa_pt/notebooks/example_experiments/test/VAE/results_epoch001/val/clustering.npy\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cl \u001b[38;5;241m=\u001b[39m \u001b[43mCluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_exp_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_exps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/tl/_cluster.py:281\u001b[0m, in \u001b[0;36mCluster.from_exp_split\u001b[0;34m(cls, exp)\u001b[0m\n\u001b[1;32m    273\u001b[0m cluster_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    274\u001b[0m cluster_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_data_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    275\u001b[0m     exp\u001b[38;5;241m.\u001b[39mdir,\n\u001b[1;32m    276\u001b[0m     exp\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults_epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    278\u001b[0m     exp\u001b[38;5;241m.\u001b[39mevaluate_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    279\u001b[0m )\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcluster_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/tl/_cluster.py:192\u001b[0m, in \u001b[0;36mCluster.__init__\u001b[0;34m(self, config, cluster_mpp, save_config)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_mpp \u001b[38;5;241m=\u001b[39m cluster_mpp\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# try to load it from disk if not already initialised\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_mpp\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# initialise annotation\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_annotation: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/tl/_cluster.py:301\u001b[0m, in \u001b[0;36mCluster.cluster_mpp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m:class:`MPPData` that is used for clustering.\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03mNone if data could not be loaded.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_mpp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_mpp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_cluster_mpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_mpp\n",
      "File \u001b[0;32m/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/tl/_cluster.py:323\u001b[0m, in \u001b[0;36mCluster._load_cluster_mpp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 323\u001b[0m     mpp_data \u001b[38;5;241m=\u001b[39m \u001b[43mMPPData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcampa_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEXPERIMENT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmpp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mumap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded cluster_mpp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmpp_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mpp_data\n",
      "File \u001b[0;32m/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/data/_data.py:192\u001b[0m, in \u001b[0;36mMPPData.from_data_dir\u001b[0;34m(cls, data_dir, data_config, mode, base_dir, keys, optional_keys, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# print(f\"!!!!!adding from {base_dir}, {data_dir}\")\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# second, add mpp_data\u001b[39;00m\n\u001b[1;32m    191\u001b[0m res_keys, res_optional_keys \u001b[38;5;241m=\u001b[39m _get_keys(keys, optional_keys, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_data_from_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptional_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_optional_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmpp_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, with base data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmpp_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_data_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m=\u001b[39m data_dir\n",
      "File \u001b[0;32m/ictstr01/home/icb/alioguz.can/projects/campa_pt/campa/data/_data.py:508\u001b[0m, in \u001b[0;36mMPPData.add_data_from_dir\u001b[0;34m(self, data_dir, keys, optional_keys, subset, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# check that obj_ids from mpp_to_add are the same / a subset of the current obj_ids\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subset:\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_obj_ids)\u001b[38;5;241m.\u001b[39missuperset(mpp_to_add\u001b[38;5;241m.\u001b[39munique_obj_ids)\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# subset self to the obj_ids in mpp_to_add\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubset(obj_ids\u001b[38;5;241m=\u001b[39mmpp_to_add\u001b[38;5;241m.\u001b[39munique_obj_ids)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cl = Cluster.from_exp_split(torch_exps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cl.config[\"leiden_resolution\"])\n",
    "cl.create_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_torch_experiments(torch_exps, mode=\"trainval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
